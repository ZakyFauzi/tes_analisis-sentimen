{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9149473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zakyf\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a94da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zakyf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zakyf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e820a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Youtube_Comment_Gibran.xlsx', header=None, names=['username', 'comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed14c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Handle non-string input\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Remove username mentions (e.g., @username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_comment'] = df['comment'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7524c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zakyf\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline('sentiment-analysis', model='w11wo/indonesian-roberta-base-sentiment-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8578fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify sentiment\n",
    "def classify_sentiment(text):\n",
    "    try:\n",
    "        # Skip empty strings\n",
    "        if not text.strip():\n",
    "            return 'netral'\n",
    "        # Truncate text to 512 tokens (max input length for RoBERTa)\n",
    "        tokens = text.split()[:512]\n",
    "        truncated_text = ' '.join(tokens)\n",
    "        result = sentiment_analyzer(truncated_text)[0]\n",
    "        label = result['label']\n",
    "        # Map model output to sentiment\n",
    "        if label == 'positive':\n",
    "            return 'positif'\n",
    "        elif label == 'negative':\n",
    "            return 'negatif'\n",
    "        else:\n",
    "            return 'netral'\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:50]}... Error: {e}\")\n",
    "        return 'netral'  # Fallback for any errors\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df['sentiment'] = df['cleaned_comment'].apply(classify_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b5ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "df[['username', 'comment', 'sentiment']].to_csv('sentiment_results.csv', index=False)\n",
    "\n",
    "# Visualization: Sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['red', 'green', 'blue'])\n",
    "plt.title('Distribusi Sentimen Komentar YouTube')\n",
    "plt.xlabel('Sentimen')\n",
    "plt.ylabel('Jumlah Komentar')\n",
    "plt.savefig('sentiment_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "267b3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ringkasan Analisis Sentimen:\n",
      "sentiment\n",
      "negatif    620\n",
      "netral     295\n",
      "positif    283\n",
      "Name: count, dtype: int64\n",
      "\n",
      "10 Kata Paling Sering Muncul:\n",
      "yg: 192\n",
      "wapres: 155\n",
      "like: 114\n",
      "ai: 109\n",
      "nya: 105\n",
      "indonesia: 101\n",
      "mas: 98\n",
      "ga: 91\n",
      "video: 88\n",
      "aja: 83\n"
     ]
    }
   ],
   "source": [
    "# Word cloud for negative comments\n",
    "negative_comments = ' '.join(df[df['sentiment'] == 'negatif']['cleaned_comment'])\n",
    "if negative_comments.strip():  # Check if there are negative comments\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=10).generate(negative_comments)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud Komentar Negatif')\n",
    "    plt.savefig('negative_wordcloud.png')\n",
    "    plt.close()\n",
    "\n",
    "# Most common words\n",
    "all_words = ' '.join(df['cleaned_comment']).split()\n",
    "word_freq = Counter(all_words).most_common(10)\n",
    "words, counts = zip(*word_freq)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(words, counts, color='orange')\n",
    "plt.title('10 Kata Paling Sering Muncul')\n",
    "plt.xlabel('Kata')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('word_frequency.png')\n",
    "plt.close()\n",
    "\n",
    "# Print summary\n",
    "print(\"Ringkasan Analisis Sentimen:\")\n",
    "print(sentiment_counts)\n",
    "print(\"\\n10 Kata Paling Sering Muncul:\")\n",
    "for word, freq in word_freq:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
